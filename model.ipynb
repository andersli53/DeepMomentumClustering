{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqneJRMFWqnWQO3vnF+c6G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cXSPfbYnjilp"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datetime import datetime, timedelta\n","from pandas.tseries.offsets import BDay\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import keras\n","from keras import ops\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense\n","from keras.metrics import Mean\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import torch\n","\n","import yfinance as yf\n","from tqdm import tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Mount google drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Colab Notebooks/Imperial MLDS/DeepTimeSeriesClustering'\n","os.chdir(path)"],"metadata":{"id":"zeandjBajqrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run real_data.ipynb\n","real_data = \"real_data.ipynb\"\n","real_data_path = os.path.join(path, real_data)\n","%run /content/drive/MyDrive/Colab\\ Notebooks/Imperial\\ MLDS/DeepTimeSeriesClustering/real_data.ipynb"],"metadata":{"id":"5PISXarGjyHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSTM-Attention-based Autoencoder"],"metadata":{"id":"FoaJJOzYj2RC"}},{"cell_type":"code","source":["from tensorflow import keras\n","from keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, MultiHeadAttention, LayerNormalization\n","from keras.regularizers import L2\n","\n","# Get latent_dim and num_heads from initial config file\n","latent_dim = config['latent_dim']\n","num_heads = config['num_heads']\n","\n","# Encoder\n","inputs = Input(shape=(time_steps, n_features)) # time_steps and n_features are extracted from real_data.ipynb, but can also be manually assigned\n","lstm1 = LSTM(latent_dim, return_sequences=True, dropout=0.1, kernel_regularizer=L2(0.03))(inputs)\n","lstm2 = LSTM(latent_dim, return_sequences=True, dropout=0.1, kernel_regularizer=L2(0.03))(lstm1)\n","attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=latent_dim//num_heads)(lstm2, lstm2) # Multi-Head Self-Attention\n","attention_output = LayerNormalization()(attention_output + lstm2)\n","encoder_output = Dense(latent_dim, kernel_regularizer=L2(0.01), activation='relu')(attention_output[:, -1, :]) # Get time_step for latent represenation\n","encoder = keras.Model(inputs, [encoder_output, attention_output], name='encoder')\n","\n","encoder.summary()\n","\n","# Decoder\n","decoder_inputs = Input(shape=(latent_dim,))\n","encoder_states = Input(shape=(time_steps, latent_dim)) # latent represenatation\n","decoded = RepeatVector(time_steps)(decoder_inputs)\n","# Architecture symmetric to encoder's\n","decoded = LSTM(latent_dim, return_sequences=True, dropout=0.1, kernel_regularizer=L2(0.03))(decoded)\n","decoded = LSTM(latent_dim, return_sequences=True, dropout=0.1, kernel_regularizer=L2(0.03))(decoded)\n","attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=latent_dim//num_heads)(decoded, encoder_states)\n","attention_output = LayerNormalization()(attention_output + decoded)\n","decoder_output = TimeDistributed(Dense(n_features))(attention_output)\n","decoder = keras.Model([decoder_inputs, encoder_states], decoder_output, name=\"decoder\")\n","\n","decoder.summary()\n"],"metadata":{"id":"ThrPxt2Bj1Bm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Deep Momentum Clustering"],"metadata":{"id":"69VLCUepmJJz"}},{"cell_type":"code","source":["from sklearn.cluster import HDBSCAN\n","from sklearn.metrics import silhouette_score\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import ops, layers\n","\n","class DeepMomentumClustering(keras.Model):\n","  def __init__(\n","      self,\n","      dataset,\n","      encoder,\n","      decoder,\n","      latent_dim,\n","      pretrain_epochs=15,\n","      gamma={\n","          'mse': 1.0,\n","          'momentum_guided': 0.3,\n","          'cluster': 1.0,\n","          'momentum': 1.0,\n","          'acceleration': 1.0\n","      },\n","      alpha=1.0,\n","      min_samples=5,\n","      update_freq=5,\n","      max_clusters=10,\n","      lookbacks=[5, 10, 20],\n","      verbose=1,\n","      **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.latent_dim = latent_dim\n","    self.epsilon = 1e-4\n","    self.verbose = verbose\n","    self.min_samples = min_samples\n","    self.gamma = gamma\n","    self.alpha = alpha\n","    self.n_components = max_clusters\n","    self.max_clusters = max_clusters\n","    self.lookbacks = lookbacks\n","    self.update_freq = update_freq\n","    self.pretrain_epochs = pretrain_epochs\n","    self.dataset = dataset\n","\n","    if self.n_components:\n","      self.cluster_centroids = tf.Variable(\n","        tf.zeros((self.n_components, self.latent_dim)),\n","        trainable=True,\n","        name='cluster_centroids'\n","      )\n","\n","    self.current_epoch = tf.Variable(0, trainable=False, dtype=tf.int32)\n","\n","    # Metrics\n","    self.loss_metric = keras.metrics.Mean(name='loss')\n","    self.mse_metric = keras.metrics.Mean(name='mse')\n","    self.cluster_metric = keras.metrics.Mean(name='cluster')\n","    self.momentum_metric = keras.metrics.Mean(name='momentum')\n","    self.acceleration_metric = keras.metrics.Mean(name='acceleration')\n","\n","    # For momentum and acceleration prediction\n","    self.momentum_layer = keras.Sequential([\n","      layers.Dense(16, activation='relu'),\n","      layers.Dense(1)\n","    ])\n","    self.acceleration_layer = keras.Sequential([\n","      layers.Dense(16, activation='relu'),\n","      layers.Dense(1)\n","    ])\n","\n","  def compute_losses(self, data):\n","    encoder_outputs = self.encoder(data)\n","    z = encoder_outputs[0] # Encoder output. Latent representations\n","    attention_output = encoder_outputs[1]\n","\n","    x_recon = self.decoder([z, attention_output]) # Reconstruct x using latent representations\n","    mse = tf.reduce_mean(tf.square(data - x_recon), axis=[1, 2]) # Compute MSE as reconstruction loss\n","\n","    mask = tf.reduce_any(data != 0, axis=[1, 2]) # Mask for non-padded samples (when not enough time steps)\n","    mse_loss = tf.where(mask, mse, 0.0)\n","    mse_loss = tf.reduce_mean(mse_loss)\n","\n","    z_masked = tf.boolean_mask(z, mask) # Mask for latent representations\n","\n","    # Clustering loss = 0 if not enough time step or during pretraining\n","    cluster_loss = tf.cond(\n","        tf.logical_and(tf.shape(z_masked)[0] > 0, tf.cast(self.current_epoch, tf.float32) >= self.pretrain_epochs),\n","        lambda: self.clustering_loss(z_masked),\n","        lambda: tf.constant(0.0, dtype=tf.float32)\n","    )\n","\n","    # Clustering weight = 0 during pretraining\n","    cluster_weight = tf.cond(\n","      tf.cast(self.current_epoch, tf.float32) < tf.cast(self.pretrain_epochs - 5, tf.float32),\n","          lambda: tf.constant(0.0, dtype=tf.float32),\n","          lambda: tf.minimum(\n","              tf.constant(self.gamma['cluster'], dtype=tf.float32) *\n","              (tf.cast(self.current_epoch, tf.float32) - tf.cast(self.pretrain_epochs - 5, tf.float32)) / 5.0,\n","              tf.constant(self.gamma['cluster'], dtype=tf.float32)\n","          ) # After pretraining, gradually increase weights according to epochs\n","    )\n","\n","    mmt_loss = self.momentum_loss(data, z)\n","    acc_loss = self.acceleration_loss(data, z)\n","\n","    loss = (self.gamma['mse'] * mse_loss +\n","                cluster_weight * cluster_loss +\n","                self.gamma['momentum'] * mmt_loss +\n","                self.gamma['acceleration'] * acc_loss) # Total loss\n","\n","    return loss, mse_loss, cluster_loss, mmt_loss, acc_loss\n","\n","  def clustering_loss(self, z):\n","    z_exp = tf.expand_dims(z, axis=1)\n","    centroids_exp = tf.expand_dims(self.cluster_centroids, axis=0)\n","    centroids_norms = tf.norm(self.cluster_centroids, axis=1)\n","    valid_centroids = tf.where(centroids_norms > 0)[:, 0]\n","    centroids_exp = tf.gather(centroids_exp, valid_centroids, axis=1)\n","    diff = z_exp - centroids_exp\n","    squared_distances = tf.reduce_sum(tf.square(diff), axis=-1)\n","    numerator = (1 + squared_distances / self.alpha) ** (-(self.alpha + 1) / 2)\n","    q = numerator / (tf.reduce_sum(numerator, axis=1, keepdims=True) + self.epsilon)\n","    f_j = tf.reduce_sum(q, axis=0)\n","    p_numerator = tf.square(q) / (f_j + self.epsilon)\n","    p = p_numerator / tf.reduce_sum(p_numerator, axis=1, keepdims=True)\n","    # KL divergence between target and cluster j's Student's t-distribution\n","    kl_div = tf.reduce_sum(\n","        p * (tf.math.log(p + self.epsilon) - tf.math.log(q + self.epsilon)),\n","        axis=1\n","    )\n","    dec_loss = tf.reduce_mean(kl_div)\n","\n","    momentum_scores = self.compute_momentum_scores(z)\n","    momentum_guided_loss = self.momentum_guidance_loss(z, q, momentum_scores)\n","    return dec_loss + self.gamma['momentum_guided'] * momentum_guided_loss\n","\n","  def compute_momentum_scores(self, z):\n","    momentum_pred = self.momentum_layer(z)[:, 0]\n","    momentum_pred = (momentum_pred - tf.reduce_sum(momentum_pred)) / (tf.math.reduce_std(momentum_pred) + self.epsilon)\n","    return momentum_pred\n","\n","  def momentum_loss(self, data, z):\n","    total_momentum_loss = 0.0\n","    returns = data[:, :, 0]\n","    volume = data[:, :, 1]\n","\n","    # Calculate momentum loss for each lookback period\n","    for lookback in self.lookbacks:\n","      volume_mean = tf.reduce_mean(volume[:, -lookback:], axis=1, keepdims=True) + self.epsilon\n","      volume_weights = volume[:, -lookback:] / volume_mean\n","      volume_weighted_cum_returns = tf.reduce_sum(returns[:, -lookback:] * volume_weights, axis=1)\n","      volume_weighted_cum_returns = (volume_weighted_cum_returns - tf.reduce_mean(volume_weighted_cum_returns)) / (tf.math.reduce_std(volume_weighted_cum_returns) + self.epsilon) # Normalizing\n","      momentum_pred = self.momentum_layer(z)[:, 0]\n","      total_momentum_loss += tf.reduce_mean(tf.square(momentum_pred  - volume_weighted_cum_returns)) / len(self.lookbacks)\n","\n","    return total_momentum_loss\n","\n","  def acceleration_loss(self, data, z):\n","    returns = data[:, :, 0]\n","    total_acceleration_loss = 0.0\n","\n","    # Calculate acceleration loss for each lookback period\n","    for lookback in self.lookbacks:\n","      momentum_t = tf.reduce_sum(returns[:, -lookback:], axis=1)\n","      momentum_t_prev = tf.reduce_sum(returns[:, -(2*lookback):-lookback], axis=1) # To sum previous momentum\n","      acceleration = momentum_t - momentum_t_prev\n","      acceleration = (acceleration - tf.reduce_mean(acceleration)) / (tf.math.reduce_std(acceleration) + self.epsilon) # Normalizing\n","      acceleration_pred = self.acceleration_layer(z)[:, 0]\n","      total_acceleration_loss += tf.reduce_mean(tf.square(acceleration_pred - acceleration)) / len(self.lookbacks)\n","\n","    return total_acceleration_loss\n","\n","  def momentum_guidance_loss(self, z, q, momentum_scores):\n","    cluster_assignments = tf.argmax(q, axis=1) # Assign temporary cluster based on q\n","    momentum_guidance_loss = tf.constant(0.0, dtype=tf.float32)\n","\n","    for cluster_idx in range(self.n_components):\n","      cluster_mask = tf.equal(cluster_assignments, cluster_idx)\n","      cluster_momentum = tf.boolean_mask(momentum_scores, cluster_mask)\n","      momentum_guidance_loss += tf.cond(\n","        tf.shape(cluster_momentum)[0] > 1,\n","        lambda: tf.clip_by_value(tf.math.reduce_variance(cluster_momentum) - 0.5 * tf.reduce_mean(cluster_momentum), 0.0, 50.0), # Clip for numerical stability\n","        lambda: tf.constant(0.0, dtype=tf.float32)\n","      )\n","\n","    return tf.reduce_mean(momentum_guidance_loss)\n","\n","  def call(self, inputs):\n","    encoder_outputs = self.encoder(inputs)\n","    z = encoder_outputs[0]\n","    attention_output = encoder_outputs[1]\n","    x_recon = self.decoder([z, attention_output])\n","    return x_recon, z\n","\n","  def train_step(self, data):\n","    with tf.GradientTape() as tape:\n","      loss, mse_loss, cluster_loss, momentum_loss, acceleration_loss = self.compute_losses(data)\n","      loss = ops.mean(loss)\n","\n","    grads = tape.gradient(loss, self.trainable_weights)\n","    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","\n","    self.loss_metric.update_state(loss)\n","    self.mse_metric.update_state(mse_loss)\n","    self.cluster_metric.update_state(cluster_loss)\n","    self.momentum_metric.update_state(momentum_loss)\n","    self.acceleration_metric.update_state(acceleration_loss)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  def test_step(self, data):\n","    loss, mse_loss, cluster_loss, momentum_loss, acceleration_loss = self.compute_losses(data)\n","    loss = ops.mean(loss)\n","    self.loss_metric.update_state(loss)\n","    self.mse_metric.update_state(mse_loss)\n","    self.cluster_metric.update_state(cluster_loss)\n","    self.momentum_metric.update_state(momentum_loss)\n","    self.acceleration_metric.update_state(acceleration_loss)\n","    return {m.name: m.result() for m in self.metrics}\n","\n","  # Update cluster centroids for training purpose\n","  def cluster_model(self, min_samples, z_np):\n","    z_np_normalized = z_np / (np.std(z_np, axis=0, keepdims=True) + self.epsilon) # Normalize for stability\n","    hdbscan = HDBSCAN(min_cluster_size=min_samples, min_samples=min_samples, cluster_selection_method='eom')\n","    labels = hdbscan.fit_predict(z_np_normalized)\n","    probabilities = hdbscan.probabilities_\n","\n","    unique_labels = np.unique(labels[labels >= 0]) # Get unique labels exclude noise\n","    n_components = len(unique_labels)\n","    if n_components == 0 or len(z_np) < min_samples:\n","      return np.zeros((1, self.latent_dim)), 1, 0.5\n","\n","    # Get cluster centroids\n","    centroids = []\n","    for label in unique_labels:\n","      cluster_points = z_np[labels == label]\n","      cluster_probs = probabilities[labels == label]\n","      weights = cluster_probs / (np.sum(cluster_probs) + self.epsilon)\n","      centroid = np.average(cluster_points, axis=0, weights=weights)\n","      centroids.append(centroid)\n","    centroids = np.array(centroids) if centroids else np.zeros((1, self.latent_dim))\n","\n","    # Random generation of centroids if fewer than max_clusters\n","    while len(centroids) < self.max_clusters:\n","      centroids = np.vstack([centroids, np.random.normal(0, 1, self.latent_dim)])\n","    centroids = centroids[:self.max_clusters] # Only extract centroids <= max_clusters\n","\n","    weighting = 0.5 + 0.5 * min(1.0, silhouette_score(z_np_normalized[labels >= 0], labels[labels >= 0]))\n","    return centroids, n_components, weighting\n","\n","  # Update centroids, n_components and weighting at every n epochs\n","  def on_epoch_end(self, epoch, logs=None):\n","    self.current_epoch.assign(epoch + 1)\n","    if self.dataset is None:\n","      return\n","\n","    z_all = []\n","    for batch in self.dataset:\n","      mask = tf.reduce_any(batch != 0, axis=[1, 2]) if len(batch.shape) > 2 else tf.reduce_any(batch != 0, axis=1)\n","      encoder_outputs = self.encoder(batch)\n","      z = encoder_outputs[0]\n","      z_masked = tf.boolean_mask(z, mask)\n","      z_all.append(z_masked.numpy())\n","\n","    z_np = np.concatenate(z_all, axis=0)\n","\n","    # Skipping if no valid data for clustering\n","    if len(z_np) == 0 or len(z_np) < self.min_samples:\n","      if self.verbose == 1:\n","        print(f\"\\nEpoch {epoch + 1}: No valid data for clustering\")\n","        return\n","\n","    # Get cluster centroids\n","    centroids, n_components, weighting = self.cluster_model(self.min_samples, z_np)\n","    new_centroids = tf.convert_to_tensor(centroids, dtype=tf.float32)\n","\n","    # If there are fewere clusters in the new centroids, pad the rest with random generation\n","    if new_centroids.shape[0] < self.max_clusters:\n","      padding_shape = (self.max_clusters - new_centroids.shape[0], self.latent_dim)\n","      padding = tf.random.normal(padding_shape, mean=0.0, stddev=1.0, dtype=tf.float32)\n","      new_centroids = tf.concat([new_centroids, padding], axis=0)\n","    elif new_centroids.shape[0] > self.max_clusters:\n","      new_centroids = new_centroids[:self.max_clusters]\n","\n","    # Assign cluster centroids on or after pretraining ends\n","    if self.current_epoch == self.pretrain_epochs:\n","      self.n_components = n_components\n","      self.cluster_centroids.assign(new_centroids[:self.max_clusters])\n","      if self.verbose == 1:\n","        print(f\"\\nEpoch {epoch + 1}: HDBSCAN initialized {n_components} centroids\")\n","    elif self.current_epoch > self.pretrain_epochs and (epoch + 1) % self.update_freq == 0:\n","      self.n_components = n_components\n","      updated_centroids = weighting * self.cluster_centroids + (1.0 - weighting) * new_centroids[:self.max_clusters]\n","      self.cluster_centroids.assign(updated_centroids)\n","      if self.verbose == 1:\n","        print(f\"\\nEpoch {epoch + 1}: HDBSCAN updated {n_components} centroids, silhouette weighting: {weighting:.3f}\")\n","\n","  @property\n","  def metrics(self):\n","    return [self.loss_metric, self.mse_metric, self.cluster_metric, self.momentum_metric, self.acceleration_metric]\n","\n","# Callback class for regular update outside of training loop\n","class ClusterCentroidUpdateCallback(keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        self.model.on_epoch_end(epoch, logs)"],"metadata":{"id":"qr0jl2eQmI4z"},"execution_count":null,"outputs":[]}]}