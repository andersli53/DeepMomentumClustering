{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIZS9dnX/SfpeoMVvo7gic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hHlRkFcSya1G"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import json\n","\n","import tensorflow as tf\n","from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, davies_bouldin_score\n","from sklearn.cluster import HDBSCAN"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Mount google drive\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/Colab Notebooks/Imperial MLDS/DeepTimeSeriesClustering'\n","os.chdir(path)"],"metadata":{"id":"VTkJcfehyoPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import functions from real_data.ipynb\n","!pip install import-ipynb\n","!pip install nbimporter\n","\n","import import_ipynb\n","import nbimporter\n","\n","from real_data import get_price, data_transformer"],"metadata":{"id":"abXpts_8yqcP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to get next run id for logging\n","def get_next_run_id():\n","  results_path = os.path.join(path, 'Results')\n","  log_path = os.path.join(results_path, 'run_logs.csv')\n","  if os.path.exists(log_path):\n","    logs = pd.read_csv(log_path, encoding='utf-8')\n","    next_run = logs['Run ID'].max() + 1\n","    return next_run\n","  else:\n","    logs = pd.DataFrame({\n","    'Run ID': [],\n","    'Number of Cluster': [],\n","    'Selected Cluster': [],\n","    'Selected Tickers': [],\n","    'Silhouette Score': [],\n","    'Davies-Bouldin Score': [],\n","    'Kruskal-Wallis p-value': [],\n","    'Selected Cluster Mean Momentum': [],\n","    'Selected Cluster Momentum Std': [],\n","    'Selected Cluster Out-of-sample Cumulative Return': [],\n","    'Selected Cluster Out-of-sample Volatility': [],\n","    'Selected Cluster Out-of-sample Sharpe Ratio': [],\n","    'Selected Cluster Outperformance': [],\n","    'Data Start Date': [],\n","    'Data End Date': [],\n","    'Test Start Date': [],\n","    'Test End Date': [],\n","    'S&P500 Cumulative Return': [],\n","    'S&P500 Volatility': [],\n","    'S&P500 Sharpe Ratio': [],\n","    })\n","    logs.to_csv(log_path, index=False)\n","    return 1\n","\n","# Function to create Results and run folders if they do not exist\n","def create_run_folder(run_id):\n","  results_path = os.path.join(path, 'Results')\n","  curr_run_folder_path = os.path.join(results_path, f\"run_{run_id}\")\n","  os.makedirs(curr_run_folder_path, exist_ok=True)\n","  return curr_run_folder_path\n","\n","# Save model, training history and parameters\n","def save_environment(run_id, model, history, config):\n","  results_path = os.path.join(path, \"Results\")\n","  curr_run_path = create_run_folder(run_id)\n","\n","  model_path = os.path.join(curr_run_path, \"model.keras\")\n","  history_path = os.path.join(curr_run_path, \"history.json\")\n","  config_path = os.path.join(curr_run_path, \"params.json\")\n","\n","  model.save(model_path)\n","\n","  with open(history_path, \"w\") as json_file:\n","    json.dump(history, json_file, indent=4)\n","\n","  with open(config_path, \"w\") as json_file:\n","    config_to_save = dict(filter(lambda x: x[0] not in ['encoder', 'decoder', 'callbacks', 'optimizer'], config.items()))\n","    json.dump(config_to_save, json_file, indent=4)\n","\n","# Save the run's statistics to csv\n","def save_run(run_id,\n","             num_cluster,\n","             selected_cluster,\n","             selected_tickers,\n","             silhouette,\n","             dbi,\n","             kw_pvalue,\n","             mean_mmt,\n","             mmt_std,\n","             out_sample_cum_return,\n","             out_sample_volatility,\n","             out_sample_sharpe,\n","             outperformance,\n","             data_start,\n","             data_end,\n","             test_start,\n","             test_end,\n","             snp500_cum_return,\n","             snp500_volatility,\n","             snp500_sharpe):\n","  results_path = os.path.join(path, 'Results')\n","  log_path = os.path.join(results_path, 'run_logs.csv')\n","  logs = pd.read_csv(log_path, encoding='utf-8')\n","  logs.loc[len(logs)] = [\n","      run_id,\n","      num_cluster,\n","      selected_cluster,\n","      selected_tickers,\n","      silhouette,\n","      dbi,\n","      kw_pvalue,\n","      mean_mmt,\n","      mmt_std,\n","      out_sample_cum_return,\n","      out_sample_volatility,\n","      out_sample_sharpe,\n","      outperformance,\n","      data_start,\n","      data_end,\n","      test_start,\n","      test_end,\n","      snp500_cum_return,\n","      snp500_volatility,\n","      snp500_sharpe\n","  ]\n","  logs.to_csv(log_path, index=False)"],"metadata":{"id":"48KIoaHZyvq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot loss\n","def plot_loss(run_id, histories, loss_name, filename=\"loss.png\", combined_losses=None):\n","    colors = plt.rcParams[\"axes.prop_cycle\"]()\n","    if combined_losses is None:\n","        combined_losses = [\"loss\"]\n","    other_losses = [l for l in loss_name if l not in combined_losses]\n","    num_charts = len(other_losses)\n","    rows = max(1, (num_charts + 1) // 2)\n","    fig = plt.figure(figsize=(15, 15))\n","\n","    # Combined losses subplot (larger, top)\n","    ax_total = plt.subplot2grid((rows + 1, 2), (0, 0), colspan=2)\n","    for i, history in enumerate(histories):\n","        c = next(colors)[\"color\"]  # Same color for all losses in one history\n","        for loss in combined_losses:\n","          title = loss.title()\n","          ylabel = 'Loss'\n","          if title == 'Loss':\n","            title = 'Total'\n","          elif title == 'Learning_Rate':\n","            title = 'Learning Rate'\n","            ylabel = 'lr'\n","          ax_total.plot(history.history[loss], color=c, label=f\"Run {i+1}\" if len(histories) > 1 else f\"{title}\")\n","    ax_total.set_title(\"Total Loss\")\n","    ax_total.set_xlabel(\"Epochs\")\n","    ax_total.set_ylabel(ylabel)\n","    if len(histories) > 1 or len(combined_losses) > 1:\n","        ax_total.legend()\n","\n","    # Grid for other losses\n","    if num_charts > 0:\n","        axes = [plt.subplot2grid((rows + 1, 2), (i // 2 + 1, i % 2)) for i in range(num_charts)]\n","        for i, loss in enumerate(other_losses):\n","          title = loss.title()\n","          ylabel = 'Loss'\n","          if title == 'Loss':\n","            title = 'Total'\n","          elif title == 'Learning_Rate':\n","            title = 'Learning Rate'\n","            ylabel = 'lr'\n","          for j, history in enumerate(histories):\n","              c = next(colors)[\"color\"]  # Same color for all losses in one history\n","              axes[i].plot(history.history[loss], color=c, label=f\"Run {j+1}\" if len(histories) > 1 else f\"{title}\")\n","          axes[i].set_title(f\"{title}\")\n","          axes[i].set_xlabel(\"Epochs\")\n","          axes[i].set_ylabel(ylabel)\n","          if len(histories) > 1:\n","              axes[i].legend()\n","\n","    plt.tight_layout()\n","\n","    results_path = os.path.join(path, \"Results\")\n","    curr_run_path = create_run_folder(run_id)\n","    img_path = os.path.join(curr_run_path, filename)\n","    plt.savefig(img_path)\n","\n","\n","    plt.show()"],"metadata":{"id":"YXkvLT4ey8gt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from matplotlib.lines import Line2D\n","\n","# Plot PCA and t-SNE\n","def plot_pca_tsne(run_id, encoded, labels, filename=\"pca_tsne.png\"):\n","  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n","  noise_mask = labels == -1\n","\n","  unique_clusters = np.unique(labels[~noise_mask])\n","  palette = sns.color_palette('deep', len(unique_clusters))\n","  cluster_colors = dict(zip(unique_clusters, palette))\n","\n","  # PCA\n","  pca = PCA(n_components=2)\n","  z_pca = pca.fit_transform(encoded)\n","  noise_scatter = ax1.scatter(z_pca[noise_mask, 0], z_pca[noise_mask, 1], c='grey', alpha=0.3, label='Noise')\n","  sns.scatterplot(x=z_pca[~noise_mask, 0], y=z_pca[~noise_mask, 1], hue=labels[~noise_mask], palette=cluster_colors, alpha=1, ax=ax1, legend=False)\n","  ax1.set_title('PCA')\n","\n","  # t-SNE\n","  tsne = TSNE(n_components=2)\n","  z_tsne = tsne.fit_transform(encoded)\n","  ax2.scatter(z_tsne[noise_mask, 0], z_tsne[noise_mask, 1], c='grey', alpha=0.3, label='Noise')\n","  sns.scatterplot(x=z_tsne[~noise_mask, 0], y=z_tsne[~noise_mask, 1], hue=labels[~noise_mask], palette=cluster_colors, alpha=1, ax=ax2, legend=False)\n","  ax2.set_title('t-SNE')\n","\n","  handles = [noise_scatter]\n","  labels_legend = ['Noise']\n","  for cluster in unique_clusters:\n","    handles.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=cluster_colors[cluster], markersize=8))\n","    labels_legend.append(str(cluster))\n","\n","  fig.legend(handles, labels_legend, loc='center right', bbox_to_anchor=(1.1, 0.5), title='Clusters')\n","\n","  plt.tight_layout()\n","\n","  results_path = os.path.join(path, \"Results\")\n","  curr_run_path = create_run_folder(run_id)\n","  img_path = os.path.join(curr_run_path, filename)\n","  plt.savefig(img_path, bbox_inches='tight')\n","\n","  plt.show()"],"metadata":{"id":"4WLVvv3czEAp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Functions to evaluate synthetic data on multiple runs\n","def synthetic_data_evaluation(run_id, num_runs, params, model, synthetic_dataset, synthetic_data_tensor, ground_truth_labels):\n","  synthetic_historical_data = []\n","  predicted_labels_list = []\n","  encoded_list = []\n","  ari_scores = []\n","  nmi_scores = []\n","  silhouette_scores = []\n","  db_scores = []\n","\n","  for run in range(num_runs):\n","    print(f\"Running Run {run+1:}\")\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","    synthetic_data_ae_clustering = model(\n","                dataset=synthetic_dataset,\n","                encoder=params[\"encoder\"],\n","                decoder=params[\"decoder\"],\n","                latent_dim=params[\"latent_dim\"],\n","                pretrain_epochs=params[\"pretrain_epochs\"],\n","                min_samples=params[\"min_samples\"],\n","                max_clusters=params[\"max_clusters\"],\n","                gamma=params[\"gamma\"],\n","                alpha=params[\"alpha\"],\n","                verbose=0\n","    )\n","    synthetic_data_ae_clustering.compile(optimizer=optimizer)\n","    synthetic_history = synthetic_data_ae_clustering.fit(synthetic_dataset, epochs=params[\"epochs\"], batch_size=params[\"batch_size\"], verbose=0, callbacks=[params[\"callbacks\"]])\n","    synthetic_historical_data.append(synthetic_history)\n","\n","    # Encode synthetic data and predict labels\n","    encoded = synthetic_data_ae_clustering.encoder(synthetic_data_tensor)[0]\n","    z_np_normalized = encoded / (np.std(encoded, axis=0, keepdims=True) + 1e-6) # Normalize latent representations for stability\n","    encoded = z_np_normalized\n","    clusterer = HDBSCAN(min_cluster_size=params['min_samples'], min_samples=params['min_samples'], cluster_selection_method='eom')\n","    predicted_labels = clusterer.fit_predict(encoded)\n","    predicted_labels_list.append(predicted_labels)\n","\n","    # Compute ARI and NMI\n","    ari_score = adjusted_rand_score(ground_truth_labels, predicted_labels)\n","    nmi_score = normalized_mutual_info_score(ground_truth_labels, predicted_labels)\n","\n","    valid_indices = predicted_labels != -1\n","    if sum(valid_indices) > 1:  # If more than 1 valid clusters (exclude noise)\n","        silhouette = silhouette_score(encoded[valid_indices], predicted_labels[valid_indices])\n","        db = davies_bouldin_score(encoded[valid_indices], predicted_labels[valid_indices])\n","    else:\n","        silhouette = np.nan\n","        db = np.nan\n","\n","    ari_scores.append(ari_score)\n","    nmi_scores.append(nmi_score)\n","    silhouette_scores.append(silhouette)\n","    db_scores.append(db)\n","    print(f\"Run {run+1} results: ARI = {ari_score:.4f}, NMI = {nmi_score:.4f}, Silhouette Score = {silhouette:.4f}, DBI = {db:.4f}\\n\")\n","\n","  mean_ari = np.mean(ari_scores)\n","  std_ari = np.std(ari_scores)\n","\n","  mean_nmi = np.mean(nmi_scores)\n","  std_nmi = np.std(nmi_scores)\n","  print(f\"Mean ARI: {mean_ari:.4f} +/- {std_ari:.4f}\")\n","  print(f\"Mean NMI: {mean_nmi:.4f} +/- {std_nmi:.4f}\")\n","\n","\n","  mean_silhouette = np.nanmean(silhouette_scores)\n","  std_silhouette = np.nanstd(silhouette_scores)\n","\n","  mean_db = np.nanmean(db_scores)\n","  std_db = np.nanstd(db_scores)\n","  print(f\"Mean Silhouette: {mean_silhouette:.4f} +/- {std_silhouette:.4f}\")\n","  print(f\"Mean DB: {mean_db:.4f} +/- {std_db:.4f}\")\n","\n","  encoded_list.append(encoded)\n","\n","  return ari_scores, nmi_scores, silhouette_scores, db_scores, encoded_list, synthetic_historical_data, predicted_labels_list"],"metadata":{"id":"HDHMIa-F_hny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute silhouette score and Davies-Bouldin index\n","def external_clustering_metrics(encoded, metadata, labels):\n","  non_noise_z_np = tf.gather(encoded, indices=metadata[metadata['Cluster']!=-1].index)\n","  silh_score = silhouette_score(non_noise_z_np, labels[labels!=-1])\n","  print(f\"Silhouette Score: {silh_score:.4f}\")\n","  db_score = davies_bouldin_score(non_noise_z_np, labels[labels!=-1])\n","  print(f\"Davies-Bouldin Score: {db_score:.4f}\")\n","  return silh_score, db_score"],"metadata":{"id":"Wu4Af8dIzyzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import kruskal\n","\n","# Compute kruskal-wallis p-value to evaluate clusters significance\n","def momentum_distribution(run_id, metadata, labels):\n","  cluster_momentum = metadata.groupby('Cluster')['Momentum'].agg(['mean', 'std', 'count']).reset_index()\n","  print(\"\\nMomentum Statistics by Cluster:\")\n","  display(cluster_momentum)\n","\n","  # Krusal-wallis p-value calculation for non noise clusters\n","  if len(np.unique(labels[labels != -1])) > 1:\n","    momentum_by_cluster = [metadata[metadata['Cluster'] == c]['Momentum'].values\n","                          for c in np.unique(labels[labels != -1])]\n","    stat, p_value = kruskal(*momentum_by_cluster)\n","    print(f\"Kruskal-Wallis p-value: {p_value:.4f}\")\n","\n","\n","  plt.figure(figsize=(10, 6))\n","  for cluster in np.unique(labels[labels != -1]):\n","      cluster_data_mmt = metadata[metadata['Cluster'] == cluster]['Momentum']\n","      plt.hist(cluster_data_mmt, bins=20, alpha=0.5, label=f'Cluster {cluster}')\n","  plt.title('Momentum Distribution by Cluster')\n","  plt.xlabel('Momentum Score')\n","  plt.ylabel('Frequency')\n","  plt.legend()\n","\n","  results_path = os.path.join(path, \"Results\")\n","  curr_run_path = create_run_folder(run_id)\n","  img_path = os.path.join(curr_run_path, \"momentum_distribution.png\")\n","  plt.savefig(img_path)\n","\n","  plt.show()\n","\n","  return p_value"],"metadata":{"id":"ATa1k-tv6k-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute normalized momentum scores over lookback periods\n","def compute_momentum_scores(data, lookbacks=[5, 10, 20]):\n","  returns = data[:, :, 0].numpy()\n","  volume = data[:, :, 1].numpy()\n","  momentum_scores = []\n","  # Aggregate momentum for each lookback period\n","  for lookback in lookbacks:\n","    volume_mean = np.mean(volume[:, -lookback:], axis=1, keepdims=True) + 1e-4\n","    volume_weights = volume[:, -lookback:] / volume_mean\n","    cum_returns = np.sum(returns[:, -lookback:] * volume_weights, axis=1)\n","    cum_returns = (cum_returns - np.mean(cum_returns)) / (np.std(cum_returns) + 1e-4)\n","    momentum_scores.append(cum_returns)\n","  return np.mean(momentum_scores, axis=0)\n","\n","# Compute normalized momentum scores over lookback periods for a dataframe\n","def compute_momentum_scores_df(df, lookbacks=[5, 10, 20]):\n","  momentum_scores = []\n","  for ticker in df['Ticker'].unique():\n","    stock_data = df[df['Ticker'] == ticker].sort_values('Date')\n","    returns = stock_data['r_close'].values\n","    volume = stock_data['norm_log_volume'].values\n","    ticker_scores = []\n","    # Aggregate momentum for each lookback period\n","    for lookback in lookbacks:\n","        if len(returns) >= lookback:\n","            volume_mean = np.mean(volume[-lookback:]) + 1e-6\n","            volume_weights = volume[-lookback:] / volume_mean\n","            cum_return = np.sum(returns[-lookback:] * volume_weights)\n","            ticker_scores.append(cum_return)\n","        else:\n","            print(f\"Warning: {ticker} has {len(returns)} time steps, insufficient for lookback {lookback}\")\n","            ticker_scores.append(0.0)\n","    momentum_scores.append(np.mean(ticker_scores) if ticker_scores else 0.0)\n","  momentum_scores = np.array(momentum_scores)\n","  momentum_scores = (momentum_scores - np.mean(momentum_scores)) / (np.std(momentum_scores) + 1e-6)\n","  return momentum_scores"],"metadata":{"id":"a2zMuJFTz3j_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute cluster metrics\n","def compute_cluster_metrics(df, out_sample_start, out_sample_end, tickers):\n","  df_copy = df.copy()\n","  df_copy = df_copy[(df_copy['Date'] >= out_sample_start) & (df_copy['Date'] <= out_sample_end)]\n","  idx = df_copy['Ticker'].isin(tickers)\n","  cluster_tickers = df_copy[idx]['Ticker'].unique()\n","  if len(cluster_tickers) == 0:\n","      return {\n","          'Cluster': -1,\n","          'Cumulative Return': 0.0,\n","          'Volatility': 0.0,\n","          'Sharpe Ratio': 0.0,\n","          'Momentum': 0.0,\n","          'Tickers': []\n","      }\n","\n","  cluster_df = df_copy[idx]\n","  daily_returns = cluster_df[cluster_df['Date'] > out_sample_start].groupby('Date')['r_close'].mean()\n","  cumulative_return = (1 + daily_returns).prod() - 1 # Cumulative returns\n","  volatility = daily_returns.std() * np.sqrt(252) # Annualized volatility\n","  sharpe_ratio = cumulative_return / (volatility + 1e-4) if volatility > 0 else 0.0 # Simplified sharpe ratio (without risk free rate)\n","  momentum = np.mean(compute_momentum_scores_df(cluster_df)) if len(cluster_tickers) > 0 else 0.0 # Average cluster's normalized momentum\n","\n","  return {\n","        'Cumulative Return': cumulative_return,\n","        'Volatility': volatility,\n","        'Sharpe Ratio': sharpe_ratio,\n","        'Momentum': momentum,\n","        'Tickers': cluster_tickers.tolist()\n","    }"],"metadata":{"id":"UhPRLYl41_GN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In-sample evaluation\n","def in_sample_evaluation(start_date, end_date, metadata):\n","  cluster_momentum = metadata.groupby('Cluster')['Momentum'].mean().reset_index()\n","  valid_clusters = cluster_momentum[cluster_momentum['Cluster'] != -1]\n","  num_clusters = len(valid_clusters)\n","  if len(valid_clusters) == 0:\n","      raise ValueError(\"No valid clusters found. Adjust HDBSCAN parameters.\")\n","\n","  # Identify high-momentum cluster\n","  high_momentum_cluster = valid_clusters.loc[valid_clusters['Momentum'].idxmax(), 'Cluster']\n","  high_momentum_tickers = metadata[metadata['Cluster'] == high_momentum_cluster]['Ticker'].unique()\n","\n","  # Display high-momentum cluster's statistics\n","  print(f\"High-momentum cluster: {high_momentum_cluster}\")\n","  print(f\"Stocks in high-momentum cluster: {high_momentum_tickers}\")\n","  print(f\"\\nIn-Sample Momentum Statistics ({start_date} to {end_date}):\")\n","  display(cluster_momentum[cluster_momentum['Cluster'] != -1][['Cluster', 'Momentum']])\n","  print(f\"\\nIn-Sample Momentum Statistics For Noise Group ({start_date} to {end_date}):\")\n","  display(cluster_momentum[cluster_momentum['Cluster'] == -1][['Cluster', 'Momentum']])\n","\n","  high_momentum_cluster_stats = metadata[metadata['Cluster'] == high_momentum_cluster].groupby('Cluster')['Momentum'].agg(['mean', 'std', 'count']).reset_index()\n","\n","  return num_clusters, high_momentum_cluster, high_momentum_tickers, high_momentum_cluster_stats['mean'].values[0], high_momentum_cluster_stats['std'].values[0]"],"metadata":{"id":"qEzMVVFw1nm1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Out-of-sample evaluation\n","def out_sample_evaluation(run_id,\n","                          in_sample_df,\n","                          out_sample_df,\n","                          data_start,\n","                          strat_start,\n","                          out_sample_start,\n","                          out_sample_end,\n","                          metadata,\n","                          high_momentum_cluster):\n","  combined_df = pd.concat([\n","      in_sample_df[in_sample_df['Date'] >= data_start],\n","      out_sample_df\n","  ]).reset_index(drop=True)\n","\n","  combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n","  combined_df = combined_df.sort_values([\"Ticker\", \"Date\"], ascending=[True, True])\n","\n","  combined_df_transformed = data_transformer(combined_df)\n","  combined_df_transformed_copy = combined_df_transformed.copy()\n","  combined_df_transformed_copy = combined_df_transformed_copy[combined_df_transformed_copy['Date'] >= strat_start]\n","\n","  cluster_metrics = []\n","  cluster_momentum = metadata.groupby('Cluster')['Momentum'].mean().reset_index()\n","  valid_clusters = cluster_momentum[cluster_momentum['Cluster'] != -1]  # Exclude noise\n","  # Compute statistics and metrics for each cluster\n","  for cluster in valid_clusters['Cluster']:\n","      cluster_tickers = metadata[metadata['Cluster'] == cluster]['Ticker'].unique()\n","      metrics = compute_cluster_metrics(combined_df_transformed_copy, out_sample_start, out_sample_end, cluster_tickers)\n","      metrics['Cluster'] = cluster\n","      cluster_metrics.append(metrics)\n","  cluster_metrics = pd.DataFrame(cluster_metrics)\n","\n","  # Retrive S&P500 from the same out-of-sample period for comparison\n","  snp500_out_sample_filename = f'snp500_out_sample_{out_sample_start.replace(\"-\", \"\")}_{out_sample_end.replace(\"-\", \"\")}.csv'\n","  snp500_out_sample_path = os.path.join(path, snp500_out_sample_filename)\n","  if not os.path.isfile(snp500_out_sample_path):\n","    snp500_df = get_price(\"SPY\", out_sample_start, out_sample_end)\n","    snp500_df['r_close'] = snp500_df['Close'].pct_change().apply(lambda x: np.log(1 + x))\n","    snp500_df.to_csv(snp500_out_sample_filename)\n","  else:\n","    snp500_df = pd.read_csv(snp500_out_sample_path, encoding='utf-8')\n","\n","  snp500_daily_returns = snp500_df['r_close']\n","  snp500_cum_return = (1 + snp500_daily_returns).prod() - 1 # Cumulative returns\n","  snp500_volatility = snp500_daily_returns.std() * np.sqrt(252) # Annualized volatility\n","  snp500_sharpe = snp500_cum_return / (snp500_volatility + 1e-4) if snp500_volatility > 0 else 0.0 # Simplified sharpe ratio (without risk free rate)\n","\n","  # If more than 10 non-noise clusters, display all. Otherwise, display only the top 10 in terms of cumulative return\n","  if len(valid_clusters) <= 10:\n","    print(f\"\\nOut-of-Sample Cluster Performance ({out_sample_start} - {out_sample_end}):\")\n","    display(cluster_metrics[['Cluster', 'Cumulative Return', 'Volatility', 'Sharpe Ratio', 'Momentum', 'Tickers']])\n","  else:\n","    print(f\"\\nTop 10 & High-Momentum Cluster Out-of-Sample Cluster Performance ({out_sample_start} - {out_sample_end}):\")\n","    cluster_metrics_top_10 = cluster_metrics.nlargest(10, 'Cumulative Return')\n","    cluster_metrics_selected = cluster_metrics[cluster_metrics['Cluster'] == high_momentum_cluster]\n","    if high_momentum_cluster not in cluster_metrics_top_10['Cluster'].unique():\n","      cluster_metrics = pd.concat([cluster_metrics_selected, cluster_metrics_top_10]).sort_values('Cumulative Return', ascending=False)\n","    cluster_metrics.reset_index(inplace=True, drop=True)\n","    display(cluster_metrics[['Cluster', 'Cumulative Return', 'Volatility', 'Sharpe Ratio', 'Momentum', 'Tickers']])\n","\n","\n","  cluster_cum_return = cluster_metrics[cluster_metrics['Cluster'] == high_momentum_cluster]['Cumulative Return'].values[0]\n","  cluster_volatility = cluster_metrics[cluster_metrics['Cluster'] == high_momentum_cluster]['Volatility'].values[0]\n","  cluster_sharpe = cluster_metrics[cluster_metrics['Cluster'] == high_momentum_cluster]['Sharpe Ratio'].iloc[0]\n","  outperformance = cluster_cum_return - snp500_cum_return\n","\n","  print(f\"\\nHigh-Momentum Cluster (Cluster {high_momentum_cluster}) Outperformance:\")\n","  print(f\"\\tCumulative Return vs. S&P 500: {outperformance:.4f}\")\n","\n","  print(f\"\\nHigh-Momentum Cluster Performance ({out_sample_start} - {out_sample_end}):\")\n","  print(f\"\\tCumulative Return: {cluster_cum_return:.4f}\")\n","  print(f\"\\tVolatility: {cluster_volatility:.4f}\")\n","  print(f\"\\tSharpe Ratio: {cluster_sharpe:.4f}\")\n","\n","  print(f\"\\nS&P 500 Performance ({out_sample_start} - {out_sample_end}):\")\n","  print(f\"\\tCumulative Return: {snp500_cum_return:.4f}\")\n","  print(f\"\\tVolatility: {snp500_volatility:.4f}\")\n","  print(f\"\\tSharpe Ratio: {snp500_sharpe:.4f}\")\n","\n","  print(\"\\nMomentum Comparison:\")\n","  momentum_comparison = cluster_momentum[cluster_momentum['Cluster'] != -1][['Cluster', 'Momentum']].rename(columns={'Momentum': 'In-Sample Momentum'})\n","  momentum_comparison = momentum_comparison.merge(\n","      cluster_metrics[['Cluster', 'Momentum']].rename(columns={'Momentum': 'Out-of-Sample Momentum'}),\n","      on='Cluster'\n","  )\n","  display(momentum_comparison)\n","\n","\n","  # Visualize returns\n","  plt.figure(figsize=(10, 6))\n","\n","  cluster_metrics['Selected'] = False\n","  cluster_metrics.loc[cluster_metrics['Cluster']==high_momentum_cluster, 'Selected'] = True\n","\n","  color_palette = {True: 'skyblue', False: 'gray'}\n","\n","  sns.barplot(cluster_metrics, x='Cluster', y='Cumulative Return', hue='Selected', palette=color_palette, dodge=False)\n","  plt.axhline(snp500_cum_return, color='orange', linestyle='--')\n","  plt.text(len(cluster_metrics)-1, snp500_cum_return, 'S&P 500', va='bottom', color='orange')\n","  plt.title(f'Out-of-Sample Cumulative Returns ({out_sample_start} - {out_sample_end})')\n","  plt.xlabel('Cluster')\n","  plt.ylabel('Cumulative Return')\n","  plt.legend(title='Selected Cluster')\n","\n","  results_path = os.path.join(path, \"Results\")\n","  curr_run_path = create_run_folder(run_id)\n","  img_path = os.path.join(curr_run_path, \"outperformance.png\")\n","  plt.savefig(img_path)\n","\n","  plt.show()\n","\n","\n","  # Visualize momentum\n","  plt.figure(figsize=(10, 6))\n","  plt.plot(momentum_comparison['Cluster'], momentum_comparison['In-Sample Momentum'], marker='o', label='In-Sample Momentum')\n","  plt.plot(momentum_comparison['Cluster'], momentum_comparison['Out-of-Sample Momentum'], marker='s', label='Out-of-Sample Momentum')\n","  plt.title('Momentum Comparison: In-Sample vs. Out-of-Sample')\n","  plt.xlabel('Cluster')\n","  plt.ylabel('Momentum Score')\n","  plt.legend()\n","  plt.show()\n","\n","  return cluster_cum_return, cluster_volatility, cluster_sharpe, snp500_cum_return, snp500_volatility, snp500_sharpe, outperformance"],"metadata":{"id":"_QWMiW6w2jSg"},"execution_count":null,"outputs":[]}]}